<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Omer Bar-Tal's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile_pic.jpg" alt="sample" width="259"> </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Omer Bar-Tal</h1>
<!--    <h3>PhD student</h3>-->
    <hr>
    <p>I am a graduate student in the Department of Computer Science and Applied Mathematics at the Weizmann Institute of Science under the supervision of Dr. <a href="https://www.weizmann.ac.il/math/dekel/home" target="_blank">
      Tali Dekel</a>. <br> My research is in computer vision and deep learning, I am particularly interested in image and video synthesis tasks.
	<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:omer.bar-tal@weizmann.ac.il">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/omerbt" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <div class="socialNetworkNav"> 
		<a href="https://www.semanticscholar.org/author/Omer-Bar-Tal/2051970111
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://twitter.com/omerbartal" target="_blank">
      <img src="AboutPageAssets/images/twitt.png" alt="sample" width="30"></a>  </div>
<!--
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/lior-yariv-3a01a690/" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>
-->
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">

    <hr class="sectionTitleRule2">

	  <!-- MultiDiffuse  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/multidiffusion.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</h2>
      		<h3 class="sectionContentSubTitle"> <b>Omer Bar-Tal*</b>, <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv*</a>, <a href="http://www.wisdom.weizmann.ac.il/~ylipman/" target="_blank">Yaron Lipman</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>2023 <b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://multidiffusion.github.io" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>

    	<!-- Text2LIVE  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/volsdf_2.gif" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/text2live.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Text2LIVE: Text-Driven Layered Image and Video Editing</h2>
      		<h3 class="sectionContentSubTitle"> <b>Omer Bar-Tal*</b>, <a href="https://www.linkedin.com/in/dolev-ofri/" target="_blank">Dolev Ofri-Amar*</a>, <a href="https://www.linkedin.com/in/rafail-fridman/" target="_blank">Rafail Fridman*</a>, <a href="https://ykasten.github.io/" target="_blank">Yoni Kasten</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>European Conference on Computer Vision (ECCV) 2022, <b style="color:darksalmon">Oral</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav"> 
		
		  <div class="dropdown"><span></span><a href="https://text2live.github.io/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Specifically, given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with new visual effects (e.g., smoke, fire) in a semantically meaningful manner. Our framework trains a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. Thus, it can perform localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.  			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2204.02491" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/Text2LIVE" target="_blank">Code</a></div>
		</aside>
	  
	  <br><br>
	  <br><br>

			  <!-- Splice  -->
	  <div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/images/teaser-splice.jpeg" style="height: 100%; width: 100%; object-fit: contain"  alt="">
<!--			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/images/Splice.mp4" type="video/mp4"> </video>-->
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Splicing ViT Features for Semantic Appearance Transfer</h2>
			<h3 class="sectionContentSubTitle"><a href="https://www.linkedin.com/in/narek-tumanyan-5a3334122/" target="_blank"> Narek Tumanyan*, </a><b>Omer Bar-Tal*</b>, <a href="https://www.weizmann.ac.il/math/bagon/home" target="_blank">Shai Bagon</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>Conference on Computer Vision and Pattern Recognition (CVPR) 2022, <b style="color:darksalmon">Oral</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://splice-vit.github.io/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We present a method for semantically transferring the visual appearance of one natural image to another. Specifically, our goal is to generate an image in which objects in a source structure image are "painted" with the visual appearance of their semantically related objects in a target appearance image. Our method works by training a generator given only a single structure/appearance image pair as input. To integrate semantic information into our framework - a pivotal component in tackling this task - our key idea is to leverage a pre-trained and fixed Vision Transformer (ViT) model which serves as an external semantic prior. Specifically, we derive novel representations of structure and appearance extracted from deep ViT features, untwisting them from the learned self-attention modules. We then establish an objective function that splices the desired structure and appearance representations, interweaving them together in the space of ViT features. Our framework, which we term "Splice", does not involve adversarial training, nor does it require any additional input information such as semantic segmentation or correspondences, and can generate high resolution results, e.g., work in HD. We demonstrate high quality results on a variety of in-the-wild image pairs, under significant variations in the number of objects, their pose and appearance.
			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://splice-vit.github.io/#paper" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/Splice" target="_blank">Code</a></div>
		</aside>
    <br><br>
	
<!-- Replicate the above Div block to add more title and company details --> 
	</section>

	  
  <hr>
  
</section>
<footer>
	<br><br>
  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
