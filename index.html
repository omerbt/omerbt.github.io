<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Omer Bar-Tal's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile1.jpg" alt="sample" width="259"> </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Omer Bar-Tal</h1>
<!--    <h3>PhD student</h3>-->
    <hr>
    <p>I am a graduate student in the Department of Computer Science and Applied Mathematics at the Weizmann Institute of Science under the supervision of Prof. <a href="https://www.weizmann.ac.il/math/dekel/home" target="_blank">
      Tali Dekel</a>. I am currently a student researcher at Google Research.<br> My research is in computer vision and deep learning, I am particularly interested in image and video synthesis tasks.
	<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:omer.bar-tal@weizmann.ac.il">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/omerbt" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <div class="socialNetworkNav"> 
		<a href="https://www.semanticscholar.org/author/Omer-Bar-Tal/2051970111
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://twitter.com/omerbartal" target="_blank">
      <img src="AboutPageAssets/images/twitt.png" alt="sample" width="30"></a>  </div>
<!--
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/lior-yariv-3a01a690/" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>
-->
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
	  <!-- SMM  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/motion_teaser.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer</h2>
      		<h3 class="sectionContentSubTitle"><a href="" target="https://www.linkedin.com/in/danah-alyce-yatim-4b15231b5/">Danah Yatim*</a>, <a href="https://www.linkedin.com/in/rafail-fridman/" target="_blank">Rafail Fridman*</a>, <b>Omer Bar-Tal</b>, <a href="https://ykasten.github.io/" target="_blank">Yoni Kasten</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>2023<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://diffusion-motion-transfer.github.io" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We present a new method for text-driven motion transfer - synthesizing a video that complies with an input text prompt describing the target objects and scene while maintaining an input video's motion and scene layout. Prior methods are confined to transferring motion across two subjects within the same or closely related object categories and are applicable for limited domains (e.g., humans). In this work, we consider a significantly more challenging setting in which the target and source objects differ drastically in shape and fine-grained motion characteristics (e.g., translating a jumping dog into a dolphin). To this end, we leverage a pre-trained and fixed text-to-video diffusion model, which provides us with generative and motion priors. The pillar of our method is a new space-time feature loss derived directly from the model. This loss guides the generation process to preserve the overall motion of the input video while complying with the target object in terms of shape and fine-grained motion traits.</p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2311.17009" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>

	  <!-- TokenFlow  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/tokenflow.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</h2>
      		<h3 class="sectionContentSubTitle"><a href="https://michalgeyer.my.canva.site/" target="_blank">Michal Geyer*</a>, <b>Omer Bar-Tal*</b>, <a href="https://www.weizmann.ac.il/math/bagon/home" target="_blank">Shai Bagon</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>International Conference on Learning Representations (ICLR) 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://diffusion-tokenflow.github.io" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The generative AI revolution has been recently expanded to videos. Nevertheless, current state-of-the-art video mod- els are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial lay- out and dynamics of the input video. Our method is based on our key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in con- junction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="" target="https://arxiv.org/abs/2307.10373">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/tokenflow" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>

	  <!-- MultiDiffuse  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/demo_nowatermark.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</h2>
      		<h3 class="sectionContentSubTitle"> <b>Omer Bar-Tal*</b>, <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv*</a>, <a href="http://www.wisdom.weizmann.ac.il/~ylipman/" target="_blank">Yaron Lipman</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>International Conference on Machine Learning (ICML) 2023<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://multidiffusion.github.io" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/MultiDiffusion" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>

    	<!-- Text2LIVE  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/volsdf_2.gif" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/teasers/text2live.mp4" type="video/mp4"> </video>
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Text2LIVE: Text-Driven Layered Image and Video Editing</h2>
      		<h3 class="sectionContentSubTitle"> <b>Omer Bar-Tal*</b>, <a href="https://www.linkedin.com/in/dolev-ofri/" target="_blank">Dolev Ofri-Amar*</a>, <a href="https://www.linkedin.com/in/rafail-fridman/" target="_blank">Rafail Fridman*</a>, <a href="https://ykasten.github.io/" target="_blank">Yoni Kasten</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>European Conference on Computer Vision (ECCV) 2022, <b style="color:darksalmon">Oral</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav"> 
		
		  <div class="dropdown"><span></span><a href="https://text2live.github.io/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Specifically, given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with new visual effects (e.g., smoke, fire) in a semantically meaningful manner. Our framework trains a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. Thus, it can perform localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.  			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2204.02491" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/Text2LIVE" target="_blank">Code</a></div>
		</aside>
	  
	  <br><br>
	  <br><br>

			  <!-- Splice  -->
	  <div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/images/teaser-splice.jpeg" style="height: 100%; width: 100%; object-fit: contain"  alt="">
<!--			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> <source src="AboutPageAssets/images/Splice.mp4" type="video/mp4"> </video>-->
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Splicing ViT Features for Semantic Appearance Transfer</h2>
			<h3 class="sectionContentSubTitle"><a href="https://www.linkedin.com/in/narek-tumanyan-5a3334122/" target="_blank"> Narek Tumanyan*, </a><b>Omer Bar-Tal*</b>, <a href="https://www.weizmann.ac.il/math/bagon/home" target="_blank">Shai Bagon</a>, <a href="https://www.weizmann.ac.il/math/dekel/" target="_blank">Tali Dekel</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>Conference on Computer Vision and Pattern Recognition (CVPR) 2022, <b style="color:darksalmon">Oral</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://splice-vit.github.io/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We present a method for semantically transferring the visual appearance of one natural image to another. Specifically, our goal is to generate an image in which objects in a source structure image are "painted" with the visual appearance of their semantically related objects in a target appearance image. Our method works by training a generator given only a single structure/appearance image pair as input. To integrate semantic information into our framework - a pivotal component in tackling this task - our key idea is to leverage a pre-trained and fixed Vision Transformer (ViT) model which serves as an external semantic prior. Specifically, we derive novel representations of structure and appearance extracted from deep ViT features, untwisting them from the learned self-attention modules. We then establish an objective function that splices the desired structure and appearance representations, interweaving them together in the space of ViT features. Our framework, which we term "Splice", does not involve adversarial training, nor does it require any additional input information such as semantic segmentation or correspondences, and can generate high resolution results, e.g., work in HD. We demonstrate high quality results on a variety of in-the-wild image pairs, under significant variations in the number of objects, their pose and appearance.
			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://splice-vit.github.io/#paper" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/omerbt/Splice" target="_blank">Code</a></div>
		</aside>
    <br><br>
	
<!-- Replicate the above Div block to add more title and company details --> 
	</section>
<section class="section2">
<h2 class="sectionTitle">Recorded Talks</h2>
	<hr>
</section>
          <p style="padding-left: 80px"><a href="https://sites.google.com/view/israel-vision-day-2022?pli=1"><b style="color: black">Israel Computer Vision day</b></a>
          <tbody>
            <tr>
              <td style="padding-left: 80px"><iframe width="80%" height="400px"
                src="https://www.youtube.com/embed/Kz7bns5qD0I">
                </iframe></td>
            </tr>
          </tbody>
<!--	<ul style="color: slategrey">-->
<!--  	<li>Splicing ViT Features: TAU Graphics Seminar 2022</li>-->
<!--  	<li>Text2LIVE: HUJI Vision Seminar 2022</li>-->
<!--	<li>Text2LIVE: Israel Computer Vision day 2022</li>-->
<!--	<li>Unveiling new priors for re-rendering images and videos: TAU Deep Learning Seminar 2023</li>-->
<!--	<li>Text2LIVE: Viz.ai 2023</li>-->
<!--</ul>-->
</section>

	</section>
<section class="section2">
<h2 class="sectionTitle">External</h2>
	<hr>
	<div style="padding-left: 80px">
<!--<p style="padding-left: 80px"><a href="https://sites.google.com/view/israel-vision-day-2022?pli=1"><b style="color: black">Israel Computer Vision day</b></a></p>-->
          <tbody>
            <tr style="padding-left: 80px;">
              <td style="padding-left: 80px"><iframe width="40%" height="200px"
                src="https://www.youtube.com/embed/8U9o5aZ2y5w">
                </iframe></td>
            </tr>
			<tr>
              <td style="padding-left: 80px"><iframe width="40%" height="200px"
                src="https://www.youtube.com/embed/TYrHBFQPMys">
                </iframe></td>
            </tr>
		  <tr>
			  <a href="https://www.reddit.com/r/StableDiffusion/comments/11fmo4g/more_control_than_controlnet_code_is_out_for/"><img src="AboutPageAssets/images/reddit.png" height="70px"></a>
		  </tr>
          </tbody>
		</div>
	</section>
</section>

<!--<section class="section2">-->
<!--<h2 class="sectionTitle">External Coverage</h2>-->
<!--	<hr>-->
<!--</section>-->
<!--          <p style="padding-left: 80px"><a href="https://www.youtube.com/watch?v=Kz7bns5qD0I"><b style="color: black">Israel Computer Vision day</b></a>-->
<!--          <tbody>-->
<!--            <tr>-->
<!--              <td style="padding-left: 80px"><iframe width="80%" height="400px"-->
<!--                src="https://www.youtube.com/embed/Kz7bns5qD0I">-->
<!--                </iframe></td>-->
<!--            </tr>-->
<!--          </tbody>-->
<!--</section>-->
<!--<hr>-->
<!--            <p style="padding-left: 80px"><a href="https://www.youtube.com/watch?v=8U9o5aZ2y5w"><b style="color: black">Two Minute Papers</b></a>-->
<!--          <tbody>-->
<!--            <tr>-->
<!--              <td style="padding-left: 80px"><iframe width="80%" height="400px"-->
<!--                src="https://www.youtube.com/embed/8U9o5aZ2y5w">-->
<!--                </iframe></td>-->
<!--            </tr>-->
<!--          </tbody>-->
<footer>
	<br><br>
  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
